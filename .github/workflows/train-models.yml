# LARUN Automated Model Training
# ================================
# Trains all 8 TinyML astronomical models on schedule or on-demand
#
# Triggers:
# - Manual dispatch with model selection
# - Weekly schedule (optional)
#
# Models:
# - EXOPLANET-001: Exoplanet transit detection
# - VSTAR-001: Variable star classification
# - FLARE-001: Stellar flare detection
# - ASTERO-001: Asteroseismology analysis
# - SUPERNOVA-001: Supernova/transient detection
# - GALAXY-001: Galaxy morphology classification
# - SPECTYPE-001: Stellar spectral type classification
# - MICROLENS-001: Microlensing event detection

name: Train Models

on:
  workflow_dispatch:  # Manual trigger
    inputs:
      model:
        description: 'Model to train (or "all" for all models)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - EXOPLANET-001
          - VSTAR-001
          - FLARE-001
          - ASTERO-001
          - SUPERNOVA-001
          - GALAXY-001
          - SPECTYPE-001
          - MICROLENS-001
      epochs:
        description: 'Training epochs'
        required: false
        default: '50'
        type: string
      samples:
        description: 'Training samples per model'
        required: false
        default: '5000'
        type: string
      skip_validation:
        description: 'Skip model validation'
        required: false
        default: false
        type: boolean
  # Optional: Weekly training on Sunday at midnight UTC
  # schedule:
  #   - cron: '0 0 * * 0'

# Prevent concurrent training runs
concurrency:
  group: training-${{ github.ref }}
  cancel-in-progress: false

env:
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================================================
  # Prepare Training
  # ==========================================================================
  prepare:
    name: Prepare Training
    runs-on: ubuntu-latest
    outputs:
      models: ${{ steps.set-models.outputs.models }}
      matrix: ${{ steps.set-matrix.outputs.matrix }}

    steps:
      - uses: actions/checkout@v4

      - name: Determine models to train
        id: set-models
        run: |
          INPUT_MODEL="${{ inputs.model || 'all' }}"

          if [ "$INPUT_MODEL" = "all" ]; then
            MODELS='["EXOPLANET-001","VSTAR-001","FLARE-001","ASTERO-001","SUPERNOVA-001","GALAXY-001","SPECTYPE-001","MICROLENS-001"]'
          else
            MODELS="[\"$INPUT_MODEL\"]"
          fi

          echo "models=$MODELS" >> $GITHUB_OUTPUT
          echo "Training models: $MODELS"

      - name: Create training matrix
        id: set-matrix
        run: |
          MODELS='${{ steps.set-models.outputs.models }}'
          echo "matrix={\"model\":$MODELS}" >> $GITHUB_OUTPUT

  # ==========================================================================
  # Train Models (Matrix)
  # ==========================================================================
  train:
    name: Train ${{ matrix.model }}
    runs-on: ubuntu-latest
    needs: prepare
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install numpy

      - name: Train model
        id: train
        run: |
          EPOCHS="${{ inputs.epochs || '50' }}"
          SAMPLES="${{ inputs.samples || '5000' }}"
          echo "Training ${{ matrix.model }} for $EPOCHS epochs with $SAMPLES samples..."

          python train_models.py --model ${{ matrix.model }} --epochs $EPOCHS --samples $SAMPLES

          # Check if model was created
          if [ -f "models/trained/${{ matrix.model }}_weights.npz" ]; then
            echo "model_created=true" >> $GITHUB_OUTPUT
          else
            echo "model_created=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload model artifact
        if: steps.train.outputs.model_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: model-${{ matrix.model }}
          path: models/trained/${{ matrix.model }}_weights.npz
          retention-days: 30

  # ==========================================================================
  # Validate Models
  # ==========================================================================
  validate:
    name: Validate Models
    runs-on: ubuntu-latest
    needs: train
    if: ${{ !inputs.skip_validation }}

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install numpy

      - name: Download all model artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-*
          path: models/trained/
          merge-multiple: true

      - name: Validate all models
        run: |
          python -c "
          import numpy as np
          import json
          from pathlib import Path
          from src.model.specialized_models import MODEL_SPECS, get_model
          from src.model.data_generators import get_generator, DatasetConfig

          results = {'models': {}, 'summary': {'passed': 0, 'failed': 0}}
          model_dir = Path('models/trained')

          for model_id in MODEL_SPECS.keys():
              weight_file = model_dir / f'{model_id}_weights.npz'
              if not weight_file.exists():
                  print(f'{model_id}: SKIPPED (no weights found)')
                  continue

              try:
                  # Load model and weights
                  model = get_model(model_id)
                  model.load(str(weight_file))

                  # Generate test data
                  generator = get_generator(model_id)
                  config = DatasetConfig(n_samples=100, seed=99)
                  X_test, y_test = generator.generate_dataset(config)

                  # Test inference
                  preds, confs = model.predict(X_test)
                  accuracy = np.mean(preds == y_test)

                  passed = accuracy > 0.1  # Basic sanity check
                  results['models'][model_id] = {
                      'passed': passed,
                      'accuracy': float(accuracy),
                      'samples_tested': len(X_test)
                  }
                  results['summary']['passed' if passed else 'failed'] += 1
                  status = 'PASSED' if passed else 'FAILED'
                  print(f'{model_id}: {status} (accuracy: {accuracy:.2%})')

              except Exception as e:
                  results['models'][model_id] = {'passed': False, 'error': str(e)}
                  results['summary']['failed'] += 1
                  print(f'{model_id}: FAILED ({e})')

          results['summary']['all_passed'] = results['summary']['failed'] == 0

          with open('validation_results.json', 'w') as f:
              json.dump(results, f, indent=2)

          print(f\"\\nTotal: {results['summary']['passed']} passed, {results['summary']['failed']} failed\")
          "

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        with:
          name: validation-results
          path: validation_results.json
          retention-days: 30

      - name: Check validation status
        run: |
          python -c "
          import json
          with open('validation_results.json') as f:
              results = json.load(f)
          if not results.get('summary', {}).get('all_passed', False):
              print('Some models failed validation!')
              for model, result in results.get('models', {}).items():
                  if not result.get('passed', False):
                      print(f'  - {model}: FAILED')
              exit(1)
          print('All models passed validation!')
          "

  # ==========================================================================
  # Bundle Models
  # ==========================================================================
  bundle:
    name: Bundle Models
    runs-on: ubuntu-latest
    needs: [train, validate]
    if: always() && needs.train.result == 'success'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install numpy

      - name: Download all model artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: model-*
          path: models/trained/
          merge-multiple: true

      - name: Create bundle
        run: |
          VERSION=$(date +%Y%m%d)-$(echo ${{ github.sha }} | cut -c1-7)
          mkdir -p dist

          # Create model bundle info
          python -c "
          import json
          import numpy as np
          from pathlib import Path
          from datetime import datetime

          model_dir = Path('models/trained')
          bundle = {
              'version': '$VERSION',
              'created': datetime.now().isoformat(),
              'models': {}
          }

          for weight_file in model_dir.glob('*_weights.npz'):
              model_id = weight_file.stem.replace('_weights', '')
              weights = np.load(weight_file)
              total_params = sum(w.size for w in weights.values())
              bundle['models'][model_id] = {
                  'file': weight_file.name,
                  'parameters': total_params,
                  'size_kb': round(weight_file.stat().st_size / 1024, 2)
              }

          with open('dist/bundle_info.json', 'w') as f:
              json.dump(bundle, f, indent=2)

          print(f'Bundle created with {len(bundle[\"models\"])} models')
          "

          # Copy all model weights to dist
          cp models/trained/*.npz dist/ 2>/dev/null || echo "No model files to copy"

          # Create training summary if exists
          cp models/trained/training_summary.json dist/ 2>/dev/null || echo "No training summary"

      - name: Upload bundle
        uses: actions/upload-artifact@v4
        with:
          name: model-bundle-${{ github.run_number }}
          path: dist/
          retention-days: 90

  # ==========================================================================
  # Summary
  # ==========================================================================
  summary:
    name: Training Summary
    runs-on: ubuntu-latest
    needs: [train, validate, bundle]
    if: always()

    steps:
      - name: Download validation results
        if: needs.validate.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: validation-results
          path: .

      - name: Generate summary
        run: |
          echo "# Training Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Step | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Training | ${{ needs.train.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validation | ${{ needs.validate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Bundle | ${{ needs.bundle.result }} |" >> $GITHUB_STEP_SUMMARY

          if [ -f validation_results.json ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Validation Details" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('validation_results.json') as f:
              results = json.load(f)
          summary = results.get('summary', {})
          print(f\"- Passed: {summary.get('passed', 0)}\")
          print(f\"- Failed: {summary.get('failed', 0)}\")
            " >> $GITHUB_STEP_SUMMARY
          fi
