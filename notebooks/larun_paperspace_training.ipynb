{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LARUN TinyML - Paperspace Gradient Training\n",
        "\n",
        "**Train the LARUN exoplanet detection model using Paperspace FREE GPU**\n",
        "\n",
        "Created by: Padmanaban Veeraragavalu (Larun Engineering)\n",
        "\n",
        "---\n",
        "\n",
        "## Paperspace Setup:\n",
        "1. Go to https://console.paperspace.com/gradient\n",
        "2. **Notebooks** → **Create** → **Free GPU (M4000)**\n",
        "3. Select **TensorFlow** runtime\n",
        "4. Upload this notebook\n",
        "5. Run all cells\n",
        "\n",
        "**Paperspace Benefits:**\n",
        "- Free M4000 GPU (6hr sessions)\n",
        "- Persistent /storage directory\n",
        "- Good for experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"\\nTensorFlow: {tf.__version__}\")\n",
        "print(f\"GPUs: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Install packages\n",
        "!pip install -q lightkurve astroquery tqdm scikit-learn\n",
        "\n",
        "import lightkurve as lk\n",
        "print(f\"Lightkurve installed: {lk.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Configuration\n",
        "import os\n",
        "\n",
        "NUM_PLANETS = 100\n",
        "NUM_NON_PLANETS = 100\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "INPUT_SIZE = 1024\n",
        "MAX_WORKERS = 8\n",
        "\n",
        "# Paperspace persistent storage\n",
        "OUTPUT_DIR = '/storage/larun_output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output: {OUTPUT_DIR} (persistent across sessions)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Fetch exoplanet hosts\n",
        "import numpy as np\n",
        "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Querying NASA Exoplanet Archive...\")\n",
        "\n",
        "planets_table = NasaExoplanetArchive.query_criteria(\n",
        "    table=\"pscomppars\",\n",
        "    select=\"hostname,disc_facility\",\n",
        "    where=\"disc_facility like '%TESS%' or disc_facility like '%Kepler%'\"\n",
        ")\n",
        "\n",
        "planet_hosts = list(set(planets_table['hostname'].data.tolist()))\n",
        "np.random.shuffle(planet_hosts)\n",
        "planet_hosts = planet_hosts[:NUM_PLANETS]\n",
        "print(f\"Found {len(planet_hosts)} exoplanet hosts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Parallel fetch function\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def fetch_lightcurve(args):\n",
        "    target, label = args\n",
        "    try:\n",
        "        search = lk.search_lightcurve(target, mission=['TESS', 'Kepler'])\n",
        "        if len(search) == 0:\n",
        "            return None\n",
        "        \n",
        "        lc = search[0].download(quality_bitmask='default')\n",
        "        lc = lc.remove_nans().normalize().remove_outliers(sigma=3)\n",
        "        flux = lc.flux.value\n",
        "        \n",
        "        if len(flux) < INPUT_SIZE:\n",
        "            flux = np.pad(flux, (0, INPUT_SIZE - len(flux)), mode='median')\n",
        "        else:\n",
        "            start = (len(flux) - INPUT_SIZE) // 2\n",
        "            flux = flux[start:start + INPUT_SIZE]\n",
        "        \n",
        "        return {'flux': flux.astype(np.float32), 'label': label, 'target': target}\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Fetch planets\n",
        "planet_data = []\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(fetch_lightcurve, (h, 1)) for h in planet_hosts]\n",
        "    for f in tqdm(as_completed(futures), total=len(futures), desc=\"Planets\"):\n",
        "        r = f.result()\n",
        "        if r: planet_data.append(r)\n",
        "\n",
        "print(f\"✓ Got {len(planet_data)} planet light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Fetch non-planets\n",
        "non_planet_tics = [f\"TIC {100000000 + i*100}\" for i in range(NUM_NON_PLANETS * 5)]\n",
        "non_planet_data = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(fetch_lightcurve, (t, 0)) for t in non_planet_tics]\n",
        "    for f in tqdm(as_completed(futures), total=len(futures), desc=\"Non-planets\"):\n",
        "        if len(non_planet_data) >= NUM_NON_PLANETS:\n",
        "            break\n",
        "        r = f.result()\n",
        "        if r: non_planet_data.append(r)\n",
        "\n",
        "non_planet_data = non_planet_data[:NUM_NON_PLANETS]\n",
        "print(f\"✓ Got {len(non_planet_data)} non-planet light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Prepare data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_data = planet_data + non_planet_data\n",
        "X = np.array([d['flux'] for d in all_data])\n",
        "y = np.array([d['label'] for d in all_data])\n",
        "\n",
        "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)\n",
        "X = X.reshape(-1, INPUT_SIZE, 1).astype(np.float32)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
        "\n",
        "np.savez(f'{OUTPUT_DIR}/training_data.npz', X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Build model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(INPUT_SIZE, 1)),\n",
        "    layers.Conv1D(32, 7, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling1D(4),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling1D(4),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "], name='larun_paperspace')\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Train\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7),\n",
        "    keras.callbacks.ModelCheckpoint(f'{OUTPUT_DIR}/best.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Evaluate and export\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Val')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Val')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.savefig(f'{OUTPUT_DIR}/history.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Export TFLite\n",
        "model.save(f'{OUTPUT_DIR}/larun_model.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite = converter.convert()\n",
        "with open(f'{OUTPUT_DIR}/larun_model.tflite', 'wb') as f:\n",
        "    f.write(tflite)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant = converter.convert()\n",
        "with open(f'{OUTPUT_DIR}/larun_model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant)\n",
        "\n",
        "print(f\"TFLite: {len(tflite)/1024:.1f} KB\")\n",
        "print(f\"INT8: {len(tflite_quant)/1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Package for download\n",
        "!cd {OUTPUT_DIR} && zip -r larun_trained.zip *.h5 *.tflite *.npz *.png\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"\\nFiles in: {OUTPUT_DIR}/\")\n",
        "print(\"Download from /storage/larun_output/ in file browser\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
