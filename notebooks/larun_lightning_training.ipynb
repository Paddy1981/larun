{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LARUN TinyML - Lightning.ai Training Notebook\n",
        "\n",
        "**Train the LARUN exoplanet detection model using Lightning.ai FREE GPU**\n",
        "\n",
        "Created by: Padmanaban Veeraragavalu (Larun Engineering)\n",
        "\n",
        "---\n",
        "\n",
        "## Lightning.ai Setup:\n",
        "1. Go to https://lightning.ai\n",
        "2. Create free account\n",
        "3. **Studios** → **New Studio** → Select **GPU**\n",
        "4. Upload this notebook or clone from GitHub\n",
        "5. Run all cells\n",
        "\n",
        "**Lightning Benefits:**\n",
        "- 22 hours/month FREE GPU\n",
        "- Persistent storage\n",
        "- Pre-configured ML environment\n",
        "- Easy sharing & collaboration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check environment\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f\"\\nPyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"TensorFlow: {tf.__version__}\")\n",
        "print(f\"TF GPUs: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Install astronomy packages\n",
        "!pip install -q lightkurve astroquery tqdm\n",
        "\n",
        "import lightkurve as lk\n",
        "print(f\"Lightkurve: {lk.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Configuration\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "NUM_PLANETS = 150\n",
        "NUM_NON_PLANETS = 150\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "INPUT_SIZE = 1024\n",
        "MAX_WORKERS = 10\n",
        "\n",
        "# Output directory (Lightning.ai persistent storage)\n",
        "OUTPUT_DIR = os.path.expanduser('~/larun_output')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Config: {NUM_PLANETS} planets, {NUM_NON_PLANETS} non-planets, {EPOCHS} epochs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Fetch exoplanet hosts from NASA\n",
        "import numpy as np\n",
        "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Querying NASA Exoplanet Archive...\")\n",
        "\n",
        "try:\n",
        "    planets_table = NasaExoplanetArchive.query_criteria(\n",
        "        table=\"pscomppars\",\n",
        "        select=\"hostname,disc_facility\",\n",
        "        where=\"disc_facility like '%TESS%' or disc_facility like '%Kepler%'\"\n",
        "    )\n",
        "    planet_hosts = list(set(planets_table['hostname'].data.tolist()))\n",
        "except Exception as e:\n",
        "    print(f\"Archive query failed: {e}\")\n",
        "    # Fallback to known hosts\n",
        "    planet_hosts = [\n",
        "        \"TOI-700\", \"TRAPPIST-1\", \"Kepler-186\", \"Kepler-442\", \"GJ 357\",\n",
        "        \"LHS 1140\", \"K2-18\", \"Kepler-22\", \"Kepler-452\", \"Kepler-62\"\n",
        "    ]\n",
        "\n",
        "np.random.shuffle(planet_hosts)\n",
        "planet_hosts = planet_hosts[:NUM_PLANETS]\n",
        "print(f\"Found {len(planet_hosts)} exoplanet hosts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Parallel data fetching\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.notebook import tqdm\n",
        "import lightkurve as lk\n",
        "\n",
        "def fetch_lightcurve(args):\n",
        "    \"\"\"Fetch and process light curve.\"\"\"\n",
        "    target, label = args\n",
        "    try:\n",
        "        search = lk.search_lightcurve(target, mission=['TESS', 'Kepler'])\n",
        "        if len(search) == 0:\n",
        "            return None\n",
        "        \n",
        "        lc = search[0].download(quality_bitmask='default')\n",
        "        lc = lc.remove_nans().normalize().remove_outliers(sigma=3)\n",
        "        flux = lc.flux.value\n",
        "        \n",
        "        # Resample to fixed size\n",
        "        if len(flux) < INPUT_SIZE:\n",
        "            flux = np.pad(flux, (0, INPUT_SIZE - len(flux)), mode='median')\n",
        "        else:\n",
        "            start = (len(flux) - INPUT_SIZE) // 2\n",
        "            flux = flux[start:start + INPUT_SIZE]\n",
        "        \n",
        "        return {'flux': flux.astype(np.float32), 'label': label, 'target': target}\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Fetch planet hosts\n",
        "print(f\"Fetching {len(planet_hosts)} exoplanet hosts...\")\n",
        "planet_data = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    tasks = [(host, 1) for host in planet_hosts]\n",
        "    futures = [executor.submit(fetch_lightcurve, t) for t in tasks]\n",
        "    \n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        result = future.result()\n",
        "        if result:\n",
        "            planet_data.append(result)\n",
        "\n",
        "print(f\"✓ Got {len(planet_data)} planet host light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Fetch non-planet stars\n",
        "print(f\"Fetching {NUM_NON_PLANETS} non-planet stars...\")\n",
        "\n",
        "non_planet_tics = [f\"TIC {100000000 + i*100}\" for i in range(NUM_NON_PLANETS * 5)]\n",
        "non_planet_data = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    tasks = [(tic, 0) for tic in non_planet_tics]\n",
        "    futures = [executor.submit(fetch_lightcurve, t) for t in tasks]\n",
        "    \n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        if len(non_planet_data) >= NUM_NON_PLANETS:\n",
        "            break\n",
        "        result = future.result()\n",
        "        if result:\n",
        "            non_planet_data.append(result)\n",
        "\n",
        "non_planet_data = non_planet_data[:NUM_NON_PLANETS]\n",
        "print(f\"✓ Got {len(non_planet_data)} non-planet light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Prepare training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_data = planet_data + non_planet_data\n",
        "print(f\"Total samples: {len(all_data)}\")\n",
        "\n",
        "X = np.array([d['flux'] for d in all_data])\n",
        "y = np.array([d['label'] for d in all_data])\n",
        "\n",
        "# Normalize\n",
        "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)\n",
        "X = X.reshape(-1, INPUT_SIZE, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
        "print(f\"Class balance: {np.bincount(y_train)}\")\n",
        "\n",
        "# Save data\n",
        "np.savez(f'{OUTPUT_DIR}/training_data.npz',\n",
        "         X_train=X_train, y_train=y_train,\n",
        "         X_val=X_val, y_val=y_val)\n",
        "print(f\"✓ Data saved to {OUTPUT_DIR}/training_data.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Build and train model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=(INPUT_SIZE, 1)),\n",
        "    \n",
        "    layers.Conv1D(32, 7, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling1D(4),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling1D(4),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(2, activation='softmax')\n",
        "], name='larun_lightning')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Train\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
        "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7, min_lr=1e-6),\n",
        "    keras.callbacks.ModelCheckpoint(f'{OUTPUT_DIR}/larun_best.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "print(\"Training...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Evaluate and visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "print(f\"\\nFinal Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"Best Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].plot(history.history['accuracy'], label='Train')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Val')\n",
        "axes[0].set_title('Accuracy')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history.history['loss'], label='Train')\n",
        "axes[1].plot(history.history['val_loss'], label='Val')\n",
        "axes[1].set_title('Loss')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.savefig(f'{OUTPUT_DIR}/training_history.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Export models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Save Keras\n",
        "model.save(f'{OUTPUT_DIR}/larun_model.h5')\n",
        "\n",
        "# TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite = converter.convert()\n",
        "with open(f'{OUTPUT_DIR}/larun_model.tflite', 'wb') as f:\n",
        "    f.write(tflite)\n",
        "\n",
        "# Quantized\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant = converter.convert()\n",
        "with open(f'{OUTPUT_DIR}/larun_model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant)\n",
        "\n",
        "print(f\"Models saved to {OUTPUT_DIR}/\")\n",
        "print(f\"  TFLite: {len(tflite)/1024:.1f} KB\")\n",
        "print(f\"  INT8: {len(tflite_quant)/1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Create download package\n",
        "import shutil\n",
        "\n",
        "# Zip everything\n",
        "!cd {OUTPUT_DIR} && zip -r larun_trained.zip *.h5 *.tflite *.npz *.png\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"\\nDownload: {OUTPUT_DIR}/larun_trained.zip\")\n",
        "print(\"\\nIn Lightning Studio: File Browser → Navigate to ~/larun_output/\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
