{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LARUN Continuous Training (Auto-Resume)\n",
        "\n",
        "**Automatically resumes training when credits reset!**\n",
        "\n",
        "This notebook:\n",
        "- Saves checkpoints every 10 epochs\n",
        "- Caches fetched data (survives restarts)\n",
        "- Auto-resumes from last checkpoint\n",
        "- Uploads best model to HuggingFace Hub\n",
        "\n",
        "---\n",
        "\n",
        "## Setup for Auto-Restart\n",
        "\n",
        "### Kaggle:\n",
        "1. **Settings** → **Accelerator** → **GPU T4 x2**\n",
        "2. **Save & Run All** (creates a version)\n",
        "3. **Schedule** → Set to run **Weekly** or **Monthly**\n",
        "\n",
        "### Manual restart:\n",
        "Just run the notebook again - it will resume from checkpoint!\n",
        "\n",
        "---\n",
        "*Larun Engineering*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - Adjust these for your needs\n",
        "# ============================================================\n",
        "\n",
        "# Training targets (increase for better model)\n",
        "TARGET_PLANETS = 300          # Total planet samples to collect\n",
        "TARGET_NON_PLANETS = 300      # Total non-planet samples\n",
        "\n",
        "# Training settings\n",
        "TOTAL_EPOCHS = 200            # Total epochs to train\n",
        "BATCH_SIZE = 64               # Batch size (64-128 for T4)\n",
        "INPUT_SIZE = 1024             # Light curve length\n",
        "MAX_WORKERS = 12              # Parallel data fetching\n",
        "\n",
        "# Checkpoint settings\n",
        "CHECKPOINT_EVERY = 10         # Save checkpoint every N epochs\n",
        "UPLOAD_TO_HUB = False         # Upload to HuggingFace Hub?\n",
        "HF_REPO = \"your-username/larun-model\"  # HuggingFace repo (if uploading)\n",
        "\n",
        "# Paths (Kaggle persistent storage)\n",
        "import os\n",
        "KAGGLE_MODE = os.path.exists('/kaggle')\n",
        "if KAGGLE_MODE:\n",
        "    BASE_DIR = '/kaggle/working'\n",
        "else:\n",
        "    BASE_DIR = './larun_output'\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "CHECKPOINT_DIR = f'{BASE_DIR}/checkpoints'\n",
        "DATA_CACHE = f'{BASE_DIR}/data_cache.npz'\n",
        "MODEL_DIR = f'{BASE_DIR}/models'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Mode: {'Kaggle' if KAGGLE_MODE else 'Local'}\")\n",
        "print(f\"Base dir: {BASE_DIR}\")\n",
        "print(f\"Target: {TARGET_PLANETS} planets + {TARGET_NON_PLANETS} non-planets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1: Check GPU and install dependencies\n",
        "# ============================================================\n",
        "!nvidia-smi\n",
        "\n",
        "!pip install -q lightkurve astroquery tqdm huggingface_hub\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"\\nTensorFlow: {tf.__version__}\")\n",
        "print(f\"GPUs: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2: Check for existing checkpoint\n",
        "# ============================================================\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load training checkpoint if exists.\"\"\"\n",
        "    checkpoint_file = f'{CHECKPOINT_DIR}/training_state.json'\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file) as f:\n",
        "            state = json.load(f)\n",
        "        print(f\"✓ Found checkpoint from {state.get('timestamp', 'unknown')}\")\n",
        "        print(f\"  Epochs completed: {state.get('epochs_completed', 0)}\")\n",
        "        print(f\"  Best accuracy: {state.get('best_accuracy', 0)*100:.2f}%\")\n",
        "        return state\n",
        "    print(\"No checkpoint found - starting fresh\")\n",
        "    return None\n",
        "\n",
        "def save_checkpoint(state):\n",
        "    \"\"\"Save training checkpoint.\"\"\"\n",
        "    state['timestamp'] = datetime.now().isoformat()\n",
        "    checkpoint_file = f'{CHECKPOINT_DIR}/training_state.json'\n",
        "    with open(checkpoint_file, 'w') as f:\n",
        "        json.dump(state, f, indent=2)\n",
        "    print(f\"✓ Checkpoint saved: epoch {state['epochs_completed']}\")\n",
        "\n",
        "checkpoint = load_checkpoint()\n",
        "START_EPOCH = checkpoint['epochs_completed'] if checkpoint else 0\n",
        "BEST_ACCURACY = checkpoint['best_accuracy'] if checkpoint else 0\n",
        "\n",
        "print(f\"\\nWill train from epoch {START_EPOCH} to {TOTAL_EPOCHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 3: Load or fetch training data\n",
        "# ============================================================\n",
        "\n",
        "def load_cached_data():\n",
        "    \"\"\"Load cached training data if exists.\"\"\"\n",
        "    if os.path.exists(DATA_CACHE):\n",
        "        data = np.load(DATA_CACHE)\n",
        "        return {\n",
        "            'planet_flux': data['planet_flux'],\n",
        "            'non_planet_flux': data['non_planet_flux'],\n",
        "            'planet_targets': data['planet_targets'].tolist(),\n",
        "            'non_planet_targets': data['non_planet_targets'].tolist()\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def save_cached_data(planet_flux, non_planet_flux, planet_targets, non_planet_targets):\n",
        "    \"\"\"Save data cache.\"\"\"\n",
        "    np.savez(DATA_CACHE,\n",
        "             planet_flux=planet_flux,\n",
        "             non_planet_flux=non_planet_flux,\n",
        "             planet_targets=np.array(planet_targets, dtype=object),\n",
        "             non_planet_targets=np.array(non_planet_targets, dtype=object))\n",
        "    print(f\"✓ Data cached: {len(planet_flux)} planets, {len(non_planet_flux)} non-planets\")\n",
        "\n",
        "cached_data = load_cached_data()\n",
        "if cached_data:\n",
        "    print(f\"✓ Loaded cached data:\")\n",
        "    print(f\"  Planets: {len(cached_data['planet_flux'])}\")\n",
        "    print(f\"  Non-planets: {len(cached_data['non_planet_flux'])}\")\n",
        "    NEED_MORE_PLANETS = len(cached_data['planet_flux']) < TARGET_PLANETS\n",
        "    NEED_MORE_NON_PLANETS = len(cached_data['non_planet_flux']) < TARGET_NON_PLANETS\n",
        "else:\n",
        "    print(\"No cached data - will fetch from NASA\")\n",
        "    NEED_MORE_PLANETS = True\n",
        "    NEED_MORE_NON_PLANETS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 4: Fetch more data if needed\n",
        "# ============================================================\n",
        "\n",
        "if NEED_MORE_PLANETS or NEED_MORE_NON_PLANETS:\n",
        "    import lightkurve as lk\n",
        "    from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "    from tqdm.notebook import tqdm\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    def fetch_lightcurve(args):\n",
        "        \"\"\"Fetch single light curve.\"\"\"\n",
        "        target, label = args\n",
        "        try:\n",
        "            search = lk.search_lightcurve(target, mission=['TESS', 'Kepler'])\n",
        "            if len(search) == 0:\n",
        "                return None\n",
        "            lc = search[0].download(quality_bitmask='default')\n",
        "            lc = lc.remove_nans().normalize().remove_outliers(sigma=3)\n",
        "            flux = lc.flux.value\n",
        "            \n",
        "            if len(flux) < INPUT_SIZE:\n",
        "                flux = np.pad(flux, (0, INPUT_SIZE - len(flux)), mode='median')\n",
        "            else:\n",
        "                start = (len(flux) - INPUT_SIZE) // 2\n",
        "                flux = flux[start:start + INPUT_SIZE]\n",
        "            \n",
        "            return {'flux': flux.astype(np.float32), 'label': label, 'target': target}\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    # Initialize from cache or empty\n",
        "    planet_flux = list(cached_data['planet_flux']) if cached_data else []\n",
        "    planet_targets = cached_data['planet_targets'] if cached_data else []\n",
        "    non_planet_flux = list(cached_data['non_planet_flux']) if cached_data else []\n",
        "    non_planet_targets = cached_data['non_planet_targets'] if cached_data else []\n",
        "    \n",
        "    # Fetch more planets if needed\n",
        "    if len(planet_flux) < TARGET_PLANETS:\n",
        "        print(f\"\\nFetching planets ({len(planet_flux)} → {TARGET_PLANETS})...\")\n",
        "        \n",
        "        planets_table = NasaExoplanetArchive.query_criteria(\n",
        "            table=\"pscomppars\",\n",
        "            select=\"hostname,disc_facility\",\n",
        "            where=\"disc_facility like '%TESS%' or disc_facility like '%Kepler%'\"\n",
        "        )\n",
        "        all_hosts = list(set(planets_table['hostname'].data.tolist()))\n",
        "        \n",
        "        # Skip already fetched\n",
        "        new_hosts = [h for h in all_hosts if h not in planet_targets]\n",
        "        np.random.shuffle(new_hosts)\n",
        "        needed = TARGET_PLANETS - len(planet_flux)\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = [executor.submit(fetch_lightcurve, (h, 1)) for h in new_hosts[:needed*2]]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Planets\"):\n",
        "                if len(planet_flux) >= TARGET_PLANETS:\n",
        "                    break\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    planet_flux.append(result['flux'])\n",
        "                    planet_targets.append(result['target'])\n",
        "        \n",
        "        print(f\"✓ Now have {len(planet_flux)} planet samples\")\n",
        "    \n",
        "    # Fetch more non-planets if needed\n",
        "    if len(non_planet_flux) < TARGET_NON_PLANETS:\n",
        "        print(f\"\\nFetching non-planets ({len(non_planet_flux)} → {TARGET_NON_PLANETS})...\")\n",
        "        \n",
        "        # Generate TIC IDs not yet fetched\n",
        "        existing_tics = set(non_planet_targets)\n",
        "        new_tics = [f\"TIC {100000000 + i*100}\" for i in range(TARGET_NON_PLANETS * 5)\n",
        "                    if f\"TIC {100000000 + i*100}\" not in existing_tics]\n",
        "        \n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            futures = [executor.submit(fetch_lightcurve, (t, 0)) for t in new_tics]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Non-planets\"):\n",
        "                if len(non_planet_flux) >= TARGET_NON_PLANETS:\n",
        "                    break\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    non_planet_flux.append(result['flux'])\n",
        "                    non_planet_targets.append(result['target'])\n",
        "        \n",
        "        print(f\"✓ Now have {len(non_planet_flux)} non-planet samples\")\n",
        "    \n",
        "    # Save updated cache\n",
        "    save_cached_data(\n",
        "        np.array(planet_flux),\n",
        "        np.array(non_planet_flux),\n",
        "        planet_targets,\n",
        "        non_planet_targets\n",
        "    )\n",
        "else:\n",
        "    planet_flux = cached_data['planet_flux']\n",
        "    non_planet_flux = cached_data['non_planet_flux']\n",
        "    print(\"Using cached data (no fetch needed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 5: Prepare training data\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine data\n",
        "X_planets = np.array(planet_flux)[:TARGET_PLANETS]\n",
        "X_non_planets = np.array(non_planet_flux)[:TARGET_NON_PLANETS]\n",
        "\n",
        "X = np.concatenate([X_planets, X_non_planets], axis=0)\n",
        "y = np.concatenate([np.ones(len(X_planets)), np.zeros(len(X_non_planets))]).astype(np.int32)\n",
        "\n",
        "# Normalize\n",
        "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)\n",
        "X = X.reshape(-1, INPUT_SIZE, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining Data:\")\n",
        "print(f\"  Total: {len(X)} samples\")\n",
        "print(f\"  Train: {len(X_train)} samples\")\n",
        "print(f\"  Val: {len(X_val)} samples\")\n",
        "print(f\"  Class balance: {np.bincount(y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 6: Build or load model\n",
        "# ============================================================\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Check for existing model checkpoint\n",
        "model_checkpoint = f'{CHECKPOINT_DIR}/model_checkpoint.h5'\n",
        "\n",
        "if os.path.exists(model_checkpoint) and START_EPOCH > 0:\n",
        "    print(f\"Loading model from checkpoint (epoch {START_EPOCH})...\")\n",
        "    model = keras.models.load_model(model_checkpoint)\n",
        "else:\n",
        "    print(\"Building new model...\")\n",
        "    \n",
        "    # Multi-GPU strategy\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f\"Devices: {strategy.num_replicas_in_sync}\")\n",
        "    \n",
        "    with strategy.scope():\n",
        "        model = keras.Sequential([\n",
        "            keras.Input(shape=(INPUT_SIZE, 1)),\n",
        "            \n",
        "            layers.Conv1D(32, 7, padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('relu'),\n",
        "            layers.MaxPooling1D(4),\n",
        "            layers.Dropout(0.25),\n",
        "            \n",
        "            layers.Conv1D(64, 5, padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('relu'),\n",
        "            layers.MaxPooling1D(4),\n",
        "            layers.Dropout(0.25),\n",
        "            \n",
        "            layers.Conv1D(128, 3, padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('relu'),\n",
        "            layers.GlobalAveragePooling1D(),\n",
        "            layers.Dropout(0.5),\n",
        "            \n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(2, activation='softmax')\n",
        "        ], name='larun_continuous')\n",
        "        \n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 7: Custom callback for checkpointing\n",
        "# ============================================================\n",
        "\n",
        "class ContinuousTrainingCallback(keras.callbacks.Callback):\n",
        "    \"\"\"Save checkpoint every N epochs for resume capability.\"\"\"\n",
        "    \n",
        "    def __init__(self, checkpoint_every=10, start_epoch=0):\n",
        "        super().__init__()\n",
        "        self.checkpoint_every = checkpoint_every\n",
        "        self.start_epoch = start_epoch\n",
        "        self.best_accuracy = BEST_ACCURACY\n",
        "        self.history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        actual_epoch = self.start_epoch + epoch + 1\n",
        "        \n",
        "        # Track history\n",
        "        for key in self.history:\n",
        "            if key in logs:\n",
        "                self.history[key].append(float(logs[key]))\n",
        "        \n",
        "        # Check for best model\n",
        "        val_acc = logs.get('val_accuracy', 0)\n",
        "        if val_acc > self.best_accuracy:\n",
        "            self.best_accuracy = val_acc\n",
        "            self.model.save(f'{MODEL_DIR}/best_model.h5')\n",
        "            print(f\"  ★ New best: {val_acc*100:.2f}%\")\n",
        "        \n",
        "        # Checkpoint every N epochs\n",
        "        if actual_epoch % self.checkpoint_every == 0:\n",
        "            # Save model\n",
        "            self.model.save(f'{CHECKPOINT_DIR}/model_checkpoint.h5')\n",
        "            \n",
        "            # Save state\n",
        "            state = {\n",
        "                'epochs_completed': actual_epoch,\n",
        "                'best_accuracy': self.best_accuracy,\n",
        "                'last_val_accuracy': val_acc,\n",
        "                'history': self.history\n",
        "            }\n",
        "            save_checkpoint(state)\n",
        "\n",
        "continuous_callback = ContinuousTrainingCallback(\n",
        "    checkpoint_every=CHECKPOINT_EVERY,\n",
        "    start_epoch=START_EPOCH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 8: Train (with auto-resume)\n",
        "# ============================================================\n",
        "\n",
        "remaining_epochs = TOTAL_EPOCHS - START_EPOCH\n",
        "\n",
        "if remaining_epochs <= 0:\n",
        "    print(f\"Training already complete! ({START_EPOCH}/{TOTAL_EPOCHS} epochs)\")\n",
        "    print(f\"Best accuracy: {BEST_ACCURACY*100:.2f}%\")\n",
        "else:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING: Epochs {START_EPOCH + 1} to {TOTAL_EPOCHS}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    callbacks = [\n",
        "        continuous_callback,\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            patience=20,\n",
        "            restore_best_weights=True,\n",
        "            monitor='val_accuracy'\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=remaining_epochs,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Final checkpoint\n",
        "    final_epoch = START_EPOCH + len(history.history['accuracy'])\n",
        "    save_checkpoint({\n",
        "        'epochs_completed': final_epoch,\n",
        "        'best_accuracy': continuous_callback.best_accuracy,\n",
        "        'status': 'completed' if final_epoch >= TOTAL_EPOCHS else 'stopped_early'\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 9: Evaluate and export\n",
        "# ============================================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load best model\n",
        "best_model_path = f'{MODEL_DIR}/best_model.h5'\n",
        "if os.path.exists(best_model_path):\n",
        "    best_model = keras.models.load_model(best_model_path)\n",
        "    val_loss, val_acc = best_model.evaluate(X_val, y_val, verbose=0)\n",
        "else:\n",
        "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "    best_model = model\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Best Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 10: Export TFLite models\n",
        "# ============================================================\n",
        "\n",
        "# Save Keras\n",
        "best_model.save(f'{MODEL_DIR}/larun_model.h5')\n",
        "\n",
        "# TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "tflite = converter.convert()\n",
        "with open(f'{MODEL_DIR}/larun_model.tflite', 'wb') as f:\n",
        "    f.write(tflite)\n",
        "\n",
        "# INT8 quantized\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant = converter.convert()\n",
        "with open(f'{MODEL_DIR}/larun_model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant)\n",
        "\n",
        "print(f\"\\nModels exported to {MODEL_DIR}/\")\n",
        "print(f\"  TFLite: {len(tflite)/1024:.1f} KB\")\n",
        "print(f\"  INT8: {len(tflite_quant)/1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 11: Upload to HuggingFace Hub (optional)\n",
        "# ============================================================\n",
        "\n",
        "if UPLOAD_TO_HUB:\n",
        "    from huggingface_hub import HfApi, login\n",
        "    \n",
        "    # Login (set HF_TOKEN in Kaggle secrets)\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        \n",
        "        api = HfApi()\n",
        "        api.upload_folder(\n",
        "            folder_path=MODEL_DIR,\n",
        "            repo_id=HF_REPO,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Update model - accuracy {val_acc*100:.2f}%\"\n",
        "        )\n",
        "        print(f\"✓ Uploaded to HuggingFace: {HF_REPO}\")\n",
        "    else:\n",
        "        print(\"Set HF_TOKEN in Kaggle secrets to enable upload\")\n",
        "else:\n",
        "    print(\"HuggingFace upload disabled (set UPLOAD_TO_HUB=True to enable)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 12: Create download package\n",
        "# ============================================================\n",
        "\n",
        "!cd {MODEL_DIR} && zip -r ../larun_trained_final.zip *.h5 *.tflite 2>/dev/null || true\n",
        "\n",
        "# List all outputs\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"OUTPUT FILES\")\n",
        "print(f\"{'='*60}\")\n",
        "for root, dirs, files in os.walk(BASE_DIR):\n",
        "    for f in files:\n",
        "        path = os.path.join(root, f)\n",
        "        size = os.path.getsize(path) / 1024\n",
        "        print(f\"  {path}: {size:.1f} KB\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"\\nTo resume training later, just run this notebook again!\")\n",
        "print(f\"Checkpoints are saved and will be automatically loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Auto-Restart Setup (Kaggle)\n",
        "\n",
        "To automatically restart training when credits reset:\n",
        "\n",
        "1. **Save this notebook** as a Kaggle Version\n",
        "2. Go to **Scheduling** (in notebook settings)\n",
        "3. Enable **Scheduled Running**\n",
        "4. Set frequency: **Weekly** or **Monthly**\n",
        "\n",
        "The notebook will:\n",
        "- Load cached data (no re-fetching)\n",
        "- Resume from last checkpoint\n",
        "- Continue training where it left off\n",
        "- Save new checkpoint when done\n",
        "\n",
        "---\n",
        "*Larun Engineering - Continuous Learning*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kaggle": {
      "accelerator": "gpu",
      "isGpuEnabled": true,
      "isInternetEnabled": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
