{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARUN TinyML - Cloud Training Notebook\n",
    "\n",
    "**Train the LARUN exoplanet detection model using free GPU resources**\n",
    "\n",
    "Created by: Padmanaban Veeraragavalu (Larun Engineering)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions:\n",
    "1. Open in Google Colab: `File > Open in Colab`\n",
    "2. Enable GPU: `Runtime > Change runtime type > GPU`\n",
    "3. Run all cells: `Runtime > Run all`\n",
    "4. Download trained model when complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check GPU availability\n",
    "!nvidia-smi\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install dependencies\n",
    "!pip install -q lightkurve astroquery astropy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clone LARUN repository\n",
    "!git clone https://github.com/Paddy1981/larun.git\n",
    "%cd larun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Configuration\n",
    "NUM_PLANETS = 100        # Number of exoplanet host stars\n",
    "NUM_NON_PLANETS = 100    # Number of non-planet stars  \n",
    "EPOCHS = 100             # Training epochs\n",
    "BATCH_SIZE = 32          # Batch size (larger with GPU)\n",
    "INPUT_SIZE = 1024        # Light curve length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Parallel Data Fetching (faster than sequential)\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import lightkurve as lk\n",
    "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Fetching confirmed exoplanet hosts from NASA...\")\n",
    "\n",
    "# Get confirmed exoplanets\n",
    "planets_table = NasaExoplanetArchive.query_criteria(\n",
    "    table=\"pscomppars\",\n",
    "    select=\"hostname,pl_name,disc_facility\",\n",
    "    where=\"disc_facility like '%TESS%' or disc_facility like '%Kepler%'\"\n",
    ")\n",
    "\n",
    "# Get unique host stars\n",
    "planet_hosts = list(set(planets_table['hostname'].data.tolist()))[:NUM_PLANETS]\n",
    "print(f\"Found {len(planet_hosts)} exoplanet host stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Parallel light curve fetching function\n",
    "def fetch_lightcurve(target, label, timeout=60):\n",
    "    \"\"\"Fetch and process a single light curve.\"\"\"\n",
    "    try:\n",
    "        search = lk.search_lightcurve(target, mission=['TESS', 'Kepler'])\n",
    "        if len(search) == 0:\n",
    "            return None\n",
    "        \n",
    "        lc = search[0].download(quality_bitmask='default')\n",
    "        lc = lc.remove_nans().normalize().remove_outliers(sigma=3)\n",
    "        \n",
    "        flux = lc.flux.value\n",
    "        \n",
    "        # Resample to fixed size\n",
    "        if len(flux) < INPUT_SIZE:\n",
    "            flux = np.pad(flux, (0, INPUT_SIZE - len(flux)), mode='median')\n",
    "        else:\n",
    "            # Take center portion\n",
    "            start = (len(flux) - INPUT_SIZE) // 2\n",
    "            flux = flux[start:start + INPUT_SIZE]\n",
    "        \n",
    "        return {'flux': flux, 'label': label, 'target': target}\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"Parallel fetch function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Fetch data in parallel (MUCH faster!)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data = []\n",
    "MAX_WORKERS = 8  # Parallel downloads\n",
    "\n",
    "print(f\"Fetching {NUM_PLANETS} exoplanet hosts (parallel)...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(fetch_lightcurve, host, 1): host for host in planet_hosts}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Planet hosts\"):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            data.append(result)\n",
    "\n",
    "print(f\"Successfully fetched {len(data)} planet host light curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fetch non-planet stars (negative examples)\n",
    "print(f\"Fetching {NUM_NON_PLANETS} non-planet stars...\")\n",
    "\n",
    "# Use TIC IDs that are known to NOT have planets\n",
    "non_planet_tics = [f\"TIC {i}\" for i in range(100000000, 100000000 + NUM_NON_PLANETS * 10, 10)]\n",
    "\n",
    "non_planet_data = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(fetch_lightcurve, tic, 0): tic for tic in non_planet_tics[:NUM_NON_PLANETS*3]}\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Non-planet stars\"):\n",
    "        if len(non_planet_data) >= NUM_NON_PLANETS:\n",
    "            break\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            non_planet_data.append(result)\n",
    "\n",
    "data.extend(non_planet_data[:NUM_NON_PLANETS])\n",
    "print(f\"Total samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Prepare training data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array([d['flux'] for d in data])\n",
    "y = np.array([d['label'] for d in data])\n",
    "\n",
    "# Reshape for CNN\n",
    "X = X.reshape(-1, INPUT_SIZE, 1).astype(np.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Build the LARUN TinyML Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_larun_model(input_shape, num_classes=2):\n",
    "    \"\"\"LARUN TinyML architecture - optimized for size and accuracy.\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Input\n",
    "        keras.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        layers.Conv1D(32, 7, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(4),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(4),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='larun_tinyml')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_larun_model((INPUT_SIZE, 1), num_classes=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Compile and train\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n",
    "    keras.callbacks.ModelCheckpoint('larun_best.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Evaluate and plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"\\nValidation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='Train')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Export to TFLite (for edge deployment)\n",
    "import tensorflow as tf\n",
    "\n",
    "# Standard TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('larun_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Quantized TFLite (smaller, faster)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant = converter.convert()\n",
    "\n",
    "with open('larun_model_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant)\n",
    "\n",
    "print(f\"Keras model: {model.count_params()} parameters\")\n",
    "print(f\"TFLite model: {len(tflite_model)/1024:.2f} KB\")\n",
    "print(f\"Quantized TFLite: {len(tflite_quant)/1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Save everything for download\n",
    "import os\n",
    "os.makedirs('larun_trained', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save('larun_trained/larun_model.h5')\n",
    "\n",
    "# Save TFLite models\n",
    "with open('larun_trained/larun_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "with open('larun_trained/larun_model_quant.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant)\n",
    "\n",
    "# Save training data\n",
    "np.savez('larun_trained/training_data.npz', X=X, y=y)\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "with open('larun_trained/history.json', 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.history.items()}, f)\n",
    "\n",
    "# Zip for download\n",
    "!zip -r larun_trained.zip larun_trained/\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "print(f\"Model saved to: larun_trained.zip\")\n",
    "print(\"\\nDownload the zip file to use with LARUN CLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Download the trained model\n",
    "from google.colab import files\n",
    "files.download('larun_trained.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
