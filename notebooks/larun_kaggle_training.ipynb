{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LARUN TinyML - Kaggle Training Notebook\n",
        "\n",
        "**Train the LARUN exoplanet detection model using Kaggle's FREE GPU**\n",
        "\n",
        "Created by: Padmanaban Veeraragavalu (Larun Engineering)\n",
        "\n",
        "---\n",
        "\n",
        "## Kaggle Setup:\n",
        "1. Click **Settings** (right sidebar) → **Accelerator** → **GPU T4 x2**\n",
        "2. Click **Run All** or run cells one by one\n",
        "3. Output files saved to `/kaggle/working/` (persists!)\n",
        "\n",
        "**Kaggle Benefits:**\n",
        "- 30 hours/week FREE GPU\n",
        "- Files persist across sessions\n",
        "- 2x T4 GPUs available\n",
        "- Pre-installed packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check GPU and Environment\n",
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"\\nTensorFlow: {tf.__version__}\")\n",
        "print(f\"GPUs: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(f\"Working dir: /kaggle/working/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Install astronomy packages (Kaggle has most pre-installed)\n",
        "!pip install -q lightkurve astroquery --upgrade\n",
        "\n",
        "# Verify\n",
        "import lightkurve as lk\n",
        "print(f\"Lightkurve: {lk.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Configuration\n",
        "# Increase these for better accuracy!\n",
        "\n",
        "NUM_PLANETS = 150        # Exoplanet host stars (increase for better model)\n",
        "NUM_NON_PLANETS = 150    # Non-planet stars for balance\n",
        "EPOCHS = 100             # Training epochs\n",
        "BATCH_SIZE = 64          # Larger batch with GPU\n",
        "INPUT_SIZE = 1024        # Light curve length\n",
        "MAX_WORKERS = 12         # Kaggle has good network!\n",
        "\n",
        "print(f\"Training config:\")\n",
        "print(f\"  Planets: {NUM_PLANETS}\")\n",
        "print(f\"  Non-planets: {NUM_NON_PLANETS}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Fetch exoplanet host list from NASA\n",
        "import numpy as np\n",
        "from astroquery.nasa_exoplanet_archive import NasaExoplanetArchive\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Querying NASA Exoplanet Archive...\")\n",
        "\n",
        "planets_table = NasaExoplanetArchive.query_criteria(\n",
        "    table=\"pscomppars\",\n",
        "    select=\"hostname,pl_name,disc_facility,pl_orbper,pl_rade\",\n",
        "    where=\"disc_facility like '%TESS%' or disc_facility like '%Kepler%'\"\n",
        ")\n",
        "\n",
        "# Get unique host stars\n",
        "planet_hosts = list(set(planets_table['hostname'].data.tolist()))\n",
        "np.random.shuffle(planet_hosts)\n",
        "planet_hosts = planet_hosts[:NUM_PLANETS]\n",
        "\n",
        "print(f\"Found {len(planet_hosts)} exoplanet host stars\")\n",
        "print(f\"Examples: {planet_hosts[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Define parallel fetch function\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import lightkurve as lk\n",
        "\n",
        "def fetch_lightcurve(args):\n",
        "    \"\"\"Fetch and process a single light curve.\"\"\"\n",
        "    target, label = args\n",
        "    try:\n",
        "        # Search for light curve\n",
        "        search = lk.search_lightcurve(target, mission=['TESS', 'Kepler'])\n",
        "        if len(search) == 0:\n",
        "            return None\n",
        "        \n",
        "        # Download and clean\n",
        "        lc = search[0].download(quality_bitmask='default')\n",
        "        lc = lc.remove_nans().normalize().remove_outliers(sigma=3)\n",
        "        \n",
        "        flux = lc.flux.value\n",
        "        \n",
        "        # Resample to fixed size\n",
        "        if len(flux) < INPUT_SIZE:\n",
        "            flux = np.pad(flux, (0, INPUT_SIZE - len(flux)), mode='median')\n",
        "        else:\n",
        "            start = (len(flux) - INPUT_SIZE) // 2\n",
        "            flux = flux[start:start + INPUT_SIZE]\n",
        "        \n",
        "        return {'flux': flux.astype(np.float32), 'label': label, 'target': target}\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "print(\"Fetch function ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Fetch planet host light curves (PARALLEL)\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(f\"Fetching {NUM_PLANETS} exoplanet hosts with {MAX_WORKERS} workers...\")\n",
        "\n",
        "planet_data = []\n",
        "tasks = [(host, 1) for host in planet_hosts]\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(fetch_lightcurve, task) for task in tasks]\n",
        "    \n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Planet hosts\"):\n",
        "        result = future.result()\n",
        "        if result is not None:\n",
        "            planet_data.append(result)\n",
        "\n",
        "print(f\"\\n✓ Fetched {len(planet_data)} planet host light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Fetch non-planet stars (negative examples)\n",
        "print(f\"Fetching {NUM_NON_PLANETS} non-planet stars...\")\n",
        "\n",
        "# Use TIC IDs known to NOT have planets\n",
        "non_planet_tics = [f\"TIC {100000000 + i*100}\" for i in range(NUM_NON_PLANETS * 5)]\n",
        "\n",
        "non_planet_data = []\n",
        "tasks = [(tic, 0) for tic in non_planet_tics]\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    futures = [executor.submit(fetch_lightcurve, task) for task in tasks]\n",
        "    \n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Non-planet stars\"):\n",
        "        if len(non_planet_data) >= NUM_NON_PLANETS:\n",
        "            break\n",
        "        result = future.result()\n",
        "        if result is not None:\n",
        "            non_planet_data.append(result)\n",
        "\n",
        "non_planet_data = non_planet_data[:NUM_NON_PLANETS]\n",
        "print(f\"\\n✓ Fetched {len(non_planet_data)} non-planet light curves\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Combine and prepare training data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_data = planet_data + non_planet_data\n",
        "print(f\"Total samples: {len(all_data)}\")\n",
        "\n",
        "X = np.array([d['flux'] for d in all_data])\n",
        "y = np.array([d['label'] for d in all_data])\n",
        "\n",
        "# Normalize\n",
        "X = (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "# Reshape for CNN: (samples, timesteps, features)\n",
        "X = X.reshape(-1, INPUT_SIZE, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nData prepared:\")\n",
        "print(f\"  Training: {len(X_train)} samples\")\n",
        "print(f\"  Validation: {len(X_val)} samples\")\n",
        "print(f\"  Class balance: {np.bincount(y_train)}\")\n",
        "print(f\"  Shape: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Save training data (Kaggle persists files!)\n",
        "np.savez('/kaggle/working/larun_training_data.npz',\n",
        "         X_train=X_train, y_train=y_train,\n",
        "         X_val=X_val, y_val=y_val,\n",
        "         targets=[d['target'] for d in all_data])\n",
        "\n",
        "print(\"✓ Training data saved to /kaggle/working/larun_training_data.npz\")\n",
        "print(\"  (This persists across Kaggle sessions!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 10: Build LARUN Model with Multi-GPU support\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Setup multi-GPU strategy (Kaggle has T4 x2)\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = keras.Sequential([\n",
        "        keras.Input(shape=(INPUT_SIZE, 1)),\n",
        "        \n",
        "        # Conv Block 1\n",
        "        layers.Conv1D(32, 7, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling1D(4),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        # Conv Block 2\n",
        "        layers.Conv1D(64, 5, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.MaxPooling1D(4),\n",
        "        layers.Dropout(0.25),\n",
        "        \n",
        "        # Conv Block 3\n",
        "        layers.Conv1D(128, 3, padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Activation('relu'),\n",
        "        layers.GlobalAveragePooling1D(),\n",
        "        layers.Dropout(0.5),\n",
        "        \n",
        "        # Dense\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(2, activation='softmax')\n",
        "    ], name='larun_kaggle')\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 11: Train with callbacks\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        patience=15, \n",
        "        restore_best_weights=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        factor=0.5, \n",
        "        patience=7, \n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        '/kaggle/working/larun_best.h5',\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Adjust batch size for multi-GPU\n",
        "effective_batch = BATCH_SIZE * strategy.num_replicas_in_sync\n",
        "print(f\"Effective batch size: {effective_batch}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING STARTED\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=effective_batch,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 12: Evaluate model\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Best Accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 13: Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "axes[0].axhline(y=max(history.history['val_accuracy']), color='g', linestyle='--', label=f'Best: {max(history.history[\"val_accuracy\"])*100:.1f}%')\n",
        "axes[0].set_title('LARUN Model Accuracy', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(history.history['loss'], label='Train', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "axes[1].set_title('LARUN Model Loss', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/training_history.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 14: Export to TFLite for edge deployment\n",
        "import tensorflow as tf\n",
        "\n",
        "# Save Keras model\n",
        "model.save('/kaggle/working/larun_model.h5')\n",
        "\n",
        "# Standard TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/kaggle/working/larun_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Quantized TFLite (INT8 for microcontrollers)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant = converter.convert()\n",
        "\n",
        "with open('/kaggle/working/larun_model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant)\n",
        "\n",
        "print(f\"\\nModel Sizes:\")\n",
        "print(f\"  Keras H5: ~{model.count_params() * 4 / 1024:.1f} KB\")\n",
        "print(f\"  TFLite: {len(tflite_model)/1024:.1f} KB\")\n",
        "print(f\"  TFLite INT8: {len(tflite_quant)/1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 15: Save training history and metadata\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "metadata = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'platform': 'kaggle',\n",
        "    'config': {\n",
        "        'num_planets': NUM_PLANETS,\n",
        "        'num_non_planets': NUM_NON_PLANETS,\n",
        "        'epochs': EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'input_size': INPUT_SIZE\n",
        "    },\n",
        "    'data': {\n",
        "        'planet_samples': len(planet_data),\n",
        "        'non_planet_samples': len(non_planet_data),\n",
        "        'train_samples': len(X_train),\n",
        "        'val_samples': len(X_val)\n",
        "    },\n",
        "    'results': {\n",
        "        'val_accuracy': float(val_acc),\n",
        "        'val_loss': float(val_loss),\n",
        "        'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
        "        'epochs_trained': len(history.history['accuracy'])\n",
        "    },\n",
        "    'history': {k: [float(v) for v in vals] for k, vals in history.history.items()}\n",
        "}\n",
        "\n",
        "with open('/kaggle/working/training_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"✓ Metadata saved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 16: Create download package\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# List all output files\n",
        "print(\"Output Files:\")\n",
        "print(\"=\"*50)\n",
        "for f in os.listdir('/kaggle/working/'):\n",
        "    size = os.path.getsize(f'/kaggle/working/{f}') / 1024\n",
        "    print(f\"  {f}: {size:.1f} KB\")\n",
        "\n",
        "# Create zip\n",
        "!cd /kaggle/working && zip -r larun_trained_kaggle.zip larun_model.h5 larun_model.tflite larun_model_int8.tflite larun_best.h5 larun_training_data.npz training_metadata.json training_history.png\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nValidation Accuracy: {val_acc*100:.2f}%\")\n",
        "print(f\"\\nFiles saved to /kaggle/working/\")\n",
        "print(\"Download 'larun_trained_kaggle.zip' from the Output tab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Instructions\n",
        "\n",
        "1. Click **Output** tab on the right sidebar\n",
        "2. Find `larun_trained_kaggle.zip`\n",
        "3. Click the **Download** button\n",
        "\n",
        "Or use the Kaggle API:\n",
        "```bash\n",
        "kaggle kernels output <your-username>/<notebook-name>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Next Steps:**\n",
        "1. Copy model files to your LARUN installation\n",
        "2. Run `/classify` to test the new model\n",
        "3. Deploy TFLite model to edge devices\n",
        "\n",
        "---\n",
        "*Larun Engineering - Astrodata*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
